{
 "cells": [
  {
   "cell_type": "code",
   "id": "7c21aee6-53b1-490a-b5a9-8b9df955b411",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "COMP5318 Assignment 1: Classification"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7bcdb4-d35a-4e00-9b74-8a086cacbe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Group number: 113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8580dc16-e7ff-4419-a529-ea789fdf760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444,0.0000,0.0000,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.4444,0.3333,0.3333,0.4444,0.6667,1.0000,0.2222,0.1111,0.0000,0\n",
      "0.2222,0.0000,0.0000,0.0000,0.1111,0.1111,0.2222,0.0000,0.0000,0\n",
      "0.5556,0.7778,0.7778,0.0000,0.2222,0.3333,0.2222,0.6667,0.0000,0\n",
      "0.3333,0.0000,0.0000,0.2222,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.7778,1.0000,1.0000,0.7778,0.6667,1.0000,0.8889,0.6667,0.0000,1\n",
      "0.0000,0.0000,0.0000,0.0000,0.1111,1.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.1111,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.0000,0.0000,0.1111,0.0000,0.0000,0.0000,0.4444,0\n",
      "0.3333,0.1111,0.0000,0.0000,0.1111,0.0000,0.1111,0.0000,0.0000,0\n",
      "\n",
      "LogR average cross-validation accuracy: 0.9642\n",
      "NB average cross-validation accuracy: 0.9585\n",
      "DT average cross-validation accuracy: 0.9385\n",
      "Bagging average cross-validation accuracy: 0.9571\n",
      "AdaBoost average cross-validation accuracy: 0.9570\n",
      "GB average cross-validation accuracy: 0.9613\n",
      "\n",
      "KNN best k: 3\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 9.695e-01\n",
      "KNN test set accuracy: 0.9543\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 0.1000\n",
      "SVM cross-validation accuracy:  0.9676\n",
      "SVM test set accuracy:  0.9714\n",
      "\n",
      "RF best n_estimators: 150\n",
      "RF best max_leaf_nodes: 6\n",
      "RF cross-validation accuracy: 0.9675\n",
      "RF test set accuracy: 0.9657\n",
      "RF test set macro average F1: 0.9628\n",
      "RF test set weighted average F1: 0.9661\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import data\n",
    "data = pd.read_csv(\"breast-cancer-wisconsin.csv\")\n",
    "\n",
    "# data = pd.read_csv(\"test-before.csv\")\n",
    "\n",
    "# Extraction class column\n",
    "class_column = data['class']\n",
    "# delete class column\n",
    "data = data.drop('class', axis=1)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Will? Replace with a missing value\n",
    "data = data.replace('?', np.nan)\n",
    "# Replace the missing value with the average value\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data = imputer.fit_transform(data)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Normalized data\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "# change class value\n",
    "class_column = class_column.replace({'class1': 0, 'class2': 1})\n",
    "\n",
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "            \n",
    "# Output the first 10 rows of the data set\n",
    "print_data(data, class_column, n_rows=10)\n",
    "print()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    model = LogisticRegression()\n",
    "    # Computational cross verification\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    model = GaussianNB()\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    model = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    model = BaggingClassifier(\n",
    "        DecisionTreeClassifier(criterion='entropy', max_depth=max_depth),\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    model = AdaBoostClassifier(\n",
    "        DecisionTreeClassifier(criterion='entropy', max_depth=max_depth),\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 60\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 6\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 60\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 6\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 60\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: {:.4f}\".format(logregClassifier(data, class_column)))\n",
    "print(\"NB average cross-validation accuracy: {:.4f}\".format(nbClassifier(data, class_column)))\n",
    "print(\"DT average cross-validation accuracy: {:.4f}\".format(dtClassifier(data, class_column)))\n",
    "print(\"Bagging average cross-validation accuracy: {:.4f}\".format(bagDTClassifier(data, class_column, bag_n_estimators, bag_max_samples, bag_max_depth)))\n",
    "print(\"AdaBoost average cross-validation accuracy: {:.4f}\".format(adaDTClassifier(data, class_column, ada_n_estimators, ada_learning_rate, ada_bag_max_depth)))\n",
    "print(\"GB average cross-validation accuracy: {:.4f}\".format(gbClassifier(data, class_column, gb_n_estimators, gb_learning_rate)))\n",
    "print()\n",
    "\n",
    "\n",
    "# KNN\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = [1, 3, 5, 7, 9]\n",
    "p = [1, 2]\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    param_grid = {'n_neighbors': k, 'p': p}\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=cvKFold)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    train_acc = grid_search.best_score_\n",
    "    test_acc = grid_search.score(X_test, y_test)\n",
    "    \n",
    "    return best_params, train_acc, test_acc\n",
    "\n",
    "# SVM\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "C = [0.01, 0.1, 1, 5, 15] \n",
    "gamma = [0.01, 0.1, 1, 10, 50]\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "    svm = SVC(kernel='rbf')\n",
    "    param_grid = {'C': C, 'gamma': gamma}\n",
    "    grid_search = GridSearchCV(svm, param_grid=param_grid, cv=cvKFold)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    train_acc = grid_search.best_score_\n",
    "    test_acc = grid_search.score(X_test, y_test)\n",
    "\n",
    "    return best_params, train_acc, test_acc\n",
    "\n",
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "n_estimators = [10, 30, 60, 100, 150]\n",
    "max_leaf_nodes = [6, 12, 18]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "    # Define parameter grid for grid search\n",
    "    param_grid = {'n_estimators': n_estimators, 'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "    # Create a random forest classifier with information gain and max_features set to 'sqrt'\n",
    "    rf = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=0)\n",
    "\n",
    "    # Use grid search to find the best hyperparameters\n",
    "    grid_search = GridSearchCV(rf, param_grid=param_grid, cv=cvKFold)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters and corresponding scores\n",
    "    best_params = grid_search.best_params_\n",
    "    train_acc = grid_search.best_score_\n",
    "    test_acc = grid_search.score(X_test, y_test)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return best_params, train_acc, test_acc, macro_f1, weighted_f1\n",
    "\n",
    "\n",
    "#print result of knn, svm, rf\n",
    "best_knn_params, best_knn_train_acc, best_knn_test_acc = bestKNNClassifier(data, class_column)\n",
    "print(\"KNN best k:\", best_knn_params['n_neighbors'])\n",
    "print(\"KNN best p:\", best_knn_params['p'])\n",
    "print(\"KNN cross-validation accuracy:\", round(best_knn_train_acc, 4))\n",
    "print(\"KNN test set accuracy:\", round(best_knn_test_acc, 4))\n",
    "print()\n",
    "\n",
    "best_params, train_acc, test_acc = bestSVMClassifier(data, class_column)\n",
    "print(\"SVM best C: {:.4f}\".format(best_params['C']))\n",
    "print(\"SVM best gamma: {:.4f}\".format(best_params['gamma']))\n",
    "print(\"SVM cross-validation accuracy: \", round(train_acc, 4))\n",
    "print(\"SVM test set accuracy: \", round(test_acc, 4))\n",
    "print()\n",
    "\n",
    "best_params, train_acc, test_acc, macro_f1, weighted_f1 = bestRFClassifier(data, class_column)\n",
    "print(\"RF best n_estimators:\", best_params['n_estimators'])\n",
    "print(\"RF best max_leaf_nodes:\", best_params['max_leaf_nodes'])\n",
    "print(\"RF cross-validation accuracy:\", round(train_acc, 4))\n",
    "print(\"RF test set accuracy:\", round(test_acc, 4))\n",
    "print(\"RF test set macro average F1:\", round(macro_f1, 4))\n",
    "print(\"RF test set weighted average F1:\", round(weighted_f1, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
